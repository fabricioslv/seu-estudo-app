# üöÄ Integra√ß√£o com Ollama - Seu Estudo

## Vis√£o Geral

Esta documenta√ß√£o descreve a integra√ß√£o avan√ßada do **Seu Estudo** com o **Ollama**, uma plataforma para execu√ß√£o de modelos de linguagem localmente. Esta integra√ß√£o permite funcionalidades avan√ßadas de IA sem depend√™ncia de servi√ßos externos pagos.

## üéØ Objetivos da Integra√ß√£o

### ‚úÖ Funcionalidades Implementadas

1. **üß† Embeddings Sem√¢nticos**
   - Modelo `nomic-embed-text` para gera√ß√£o de embeddings
   - Busca sem√¢ntica inteligente em livros did√°ticos
   - Similaridade de conte√∫do baseada em contexto

2. **üí¨ Chat com Livros**
   - Modelo `llama3.1:8b` para conversa√ß√£o natural
   - Contexto baseado no conte√∫do real dos livros
   - Respostas educativas e personalizadas

3. **üìö Processamento Inteligente**
   - Extra√ß√£o autom√°tica de conceitos-chave
   - Gera√ß√£o de resumos contextuais
   - Cria√ß√£o de quest√µes personalizadas

4. **üîç Busca Avan√ßada**
   - Busca sem√¢ntica com filtros
   - Recomenda√ß√µes baseadas em desempenho
   - Sistema de busca conversacional

## üèóÔ∏è Arquitetura da Integra√ß√£o

### Componentes Principais

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Frontend      ‚îÇ    ‚îÇ     Backend      ‚îÇ    ‚îÇ     Ollama      ‚îÇ
‚îÇ   (React)       ‚îÇ‚óÑ‚îÄ‚îÄ‚ñ∫‚îÇ   (Node.js)      ‚îÇ‚óÑ‚îÄ‚îÄ‚ñ∫‚îÇ   (Local AI)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ                       ‚îÇ                       ‚îÇ
         ‚ñº                       ‚ñº                       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Busca IA      ‚îÇ    ‚îÇ  Embedding       ‚îÇ    ‚îÇ   Modelos       ‚îÇ
‚îÇ   Interface     ‚îÇ    ‚îÇ  Service         ‚îÇ    ‚îÇ   Locais        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Fluxo de Dados

1. **Usu√°rio faz consulta** ‚Üí Frontend captura e envia
2. **Backend processa** ‚Üí Valida e prepara dados
3. **Ollama processa** ‚Üí Gera embeddings ou respostas
4. **Resultado retorna** ‚Üí Interface exibe resposta

## üìã Instala√ß√£o e Configura√ß√£o

### 1. Instala√ß√£o do Ollama

```bash
# Download e instala√ß√£o (Linux/Mac)
curl -fsSL https://ollama.ai/install.sh | sh

# No Windows
# Download manual em: https://ollama.ai/download
```

### 2. Inicializa√ß√£o e Modelos

```bash
# Iniciar servi√ßo Ollama
ollama serve

# Puxar modelos necess√°rios (em outro terminal)
ollama pull nomic-embed-text  # Para embeddings
ollama pull llama3.1:8b        # Para chat e an√°lise
ollama pull llama3.1:70b       # Modelo maior (opcional)
```

### 3. Verifica√ß√£o da Instala√ß√£o

```bash
# Verificar se Ollama est√° rodando
curl http://localhost:11434/api/tags

# Listar modelos instalados
ollama list

# Testar modelo
ollama run llama3.1:8b "Ol√°, me explique fun√ß√µes matem√°ticas"
```

## üîß Configura√ß√£o do Backend

### Vari√°veis de Ambiente

```bash
# .env
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_EMBEDDING_MODEL=nomic-embed-text
OLLAMA_CHAT_MODEL=llama3.1:8b
OLLAMA_TIMEOUT=30000
OLLAMA_MAX_RETRIES=3
```

### Inicializa√ß√£o Autom√°tica

```javascript
// services/ollamaService.js
class OllamaService {
    async initializeModels() {
        const models = ['nomic-embed-text', 'llama3.1:8b'];

        for (const model of models) {
            try {
                await this.pullModel(model);
                console.log(`‚úÖ Modelo ${model} inicializado`);
            } catch (error) {
                console.error(`‚ùå Erro ao inicializar ${model}:`, error);
            }
        }
    }
}
```

## üìö Funcionalidades Detalhadas

### 3.1 Embeddings Sem√¢nticos

#### Gera√ß√£o de Embeddings
```javascript
// Gera embedding para texto
const embedding = await ollamaService.generateEmbedding("Fun√ß√µes lineares na matem√°tica");

console.log(embedding); // Array de 768 n√∫meros (nomic-embed-text)
```

#### Busca Sem√¢ntica
```javascript
// Busca conte√∫do similar
const results = await embeddingService.semanticSearch(
    "Como resolver equa√ß√µes lineares?",
    livroId, // opcional
    5 // topK
);

results.forEach(result => {
    console.log(`Similaridade: ${result.similarity}`);
    console.log(`Texto: ${result.chunk_text}`);
});
```

#### C√°lculo de Similaridade
```javascript
cosineSimilarity(vecA, vecB) {
    // Implementa√ß√£o do c√°lculo de similaridade cosseno
    const dotProduct = vecA.reduce((acc, val, i) => acc + val * vecB[i], 0);
    const normA = Math.sqrt(vecA.reduce((acc, val) => acc + val * val, 0));
    const normB = Math.sqrt(vecB.reduce((acc, val) => acc + val * val, 0));

    return dotProduct / (normA * normB);
}
```

### 3.2 Chat com Livros

#### Implementa√ß√£o B√°sica
```javascript
const resposta = await ollamaService.chatWithBook(
    conteudoDoLivro,
    perguntaDoUsuario
);

console.log(resposta); // Resposta educativa baseada no livro
```

#### Contexto Din√¢mico
```javascript
// Buscar contexto relevante automaticamente
const contextoRelevante = await embeddingService.getSimilarContent(
    pergunta,
    livroId,
    3 // n√∫mero de chunks
);

// Gerar resposta baseada no contexto
const resposta = await ollamaService.chatWithBook(
    contextoRelevante.map(c => c.chunk_text).join('\n'),
    pergunta
);
```

### 3.3 Processamento de Livros

#### Extra√ß√£o com IA
```javascript
// Processar cap√≠tulo com IA
const conceitos = await ollamaService.extractKeyConcepts(conteudo);
const resumo = await ollamaService.summarizeContent(conteudo);
const questoes = await ollamaService.generateQuestions(conteudo, 'medio', 5);
```

#### Gera√ß√£o de Embeddings
```javascript
// Dividir livro em chunks
const chunks = await embeddingService.createChunksFromBookContent(
    livroContent,
    1000, // tamanho do chunk
    200   // overlap
);

// Gerar embeddings
const results = await embeddingService.generateBookEmbeddings(livroId, chunks);
```

## üéÆ Uso Pr√°tico

### 4.1 P√°gina de Busca Inteligente

#### Funcionalidades Dispon√≠veis
- **Busca sem√¢ntica** em toda a biblioteca
- **Chat conversacional** com livros espec√≠ficos
- **Filtros avan√ßados** por mat√©ria e dificuldade
- **Recomenda√ß√µes personalizadas**
- **Interface responsiva** e moderna

#### Exemplo de Uso
```javascript
// Busca sem√¢ntica
const searchResults = await fetch('/api/livros/search', {
    method: 'POST',
    body: JSON.stringify({
        query: "explique fun√ß√µes quadr√°ticas",
        topK: 10
    })
});

// Chat com livro
const chatResponse = await fetch(`/api/livros/${livroId}/chat`, {
    method: 'POST',
    body: JSON.stringify({
        question: "Como calcular o v√©rtice de uma par√°bola?"
    })
});
```

### 4.2 Recomenda√ß√µes Personalizadas

```javascript
// Obter recomenda√ß√µes baseadas no desempenho
const recomendacoes = await fetch('/api/livros/recommendations');

// Baseado em:
// - Mat√©rias com melhor desempenho
// - Tempo dedicado a cada assunto
// - Progress√£o de aprendizado
```

## üìä Monitoramento e Performance

### 5.1 M√©tricas de Uso

```javascript
// Estat√≠sticas da integra√ß√£o
const stats = {
    ollama: ollamaService.getStatus(),
    embeddings: embeddingService.getStats(),
    performance: {
        tempo_medio_resposta: 1.2, // segundos
        taxa_sucesso: 0.95,
        embeddings_gerados: 15420,
        consultas_processadas: 8750
    }
};
```

### 5.2 Logs e Debugging

```javascript
// Logs estruturados
const logger = {
    timestamp: new Date().toISOString(),
    servico: 'ollama-integration',
    acao: 'chat_with_book',
    livro_id: livroId,
    pergunta: question.substring(0, 100),
    tempo_resposta: 1250, // ms
    sucesso: true
};
```

## üîß Troubleshooting

### 6.1 Problemas Comuns

#### Ollama n√£o Conecta
```bash
# Verificar se Ollama est√° rodando
curl http://localhost:11434/api/tags

# Reiniciar servi√ßo
ollama serve

# Verificar logs
journalctl -u ollama -f  # Linux
```

#### Modelos n√£o Encontrados
```bash
# Puxar modelo novamente
ollama pull nomic-embed-text
ollama pull llama3.1:8b

# Listar modelos dispon√≠veis
ollama list

# Remover e reinstalar modelo
ollama rm modelo-problematico
ollama pull modelo-problematico
```

#### Mem√≥ria Insuficiente
```bash
# Configurar Ollama para usar menos mem√≥ria
OLLAMA_MAX_LOADED=1 OLLAMA_MAX_QUEUE=256 ollama serve

# Ou usar modelo menor
ollama pull llama3.1:8b  # ao inv√©s de 70b
```

### 6.2 Debugging

#### Verificar Embeddings
```javascript
// Teste de gera√ß√£o de embeddings
const testEmbedding = await ollamaService.generateEmbedding("teste");
console.log(`Embedding gerado: ${testEmbedding.length} dimens√µes`);
```

#### Testar Chat
```javascript
// Teste de chat b√°sico
const resposta = await ollamaService.chatWithBook(
    "Matem√°tica b√°sica inclui n√∫meros e opera√ß√µes.",
    "O que √© matem√°tica b√°sica?"
);
console.log("Resposta:", resposta);
```

## üöÄ Performance e Otimiza√ß√£o

### 7.1 Estrat√©gias de Cache

```javascript
// Cache de embeddings
const cacheKey = `embedding:${text}`;
let embedding = await cache.get(cacheKey);

if (!embedding) {
    embedding = await ollamaService.generateEmbedding(text);
    await cache.set(cacheKey, embedding, 3600); // 1 hora
}
```

### 7.2 Processamento Ass√≠ncrono

```javascript
// Processar livros em background
async function processBooksInBackground() {
    const livros = await getLivrosPendentes();

    for (const livro of livros) {
        // N√£o aguardar resultado, processar em paralelo
        processarLivroComIA(livro.id).catch(console.error);
    }
}
```

### 7.3 Otimiza√ß√£o de Prompts

```javascript
// Prompt otimizado para respostas r√°pidas
const promptOtimizado = `
Baseado no conte√∫do: ${conteudo}
Pergunta: ${pergunta}
Resposta direta e educativa:`;

// Configura√ß√µes de gera√ß√£o
const options = {
    temperature: 0.3,  // Mais determin√≠stico
    top_p: 0.9,        // Melhor qualidade
    num_predict: 200   // Limitar tamanho
};
```

## üîí Seguran√ßa e Privacidade

### 8.1 Processamento Local

- **‚úÖ Dados ficam locais** - N√£o envia para servi√ßos externos
- **‚úÖ Privacidade garantida** - Conte√∫do estudantil protegido
- **‚úÖ Controle total** - Voc√™ decide quais dados processar

### 8.2 Sanitiza√ß√£o de Dados

```javascript
// Limpar conte√∫do antes de enviar para IA
function sanitizeContent(content) {
    return content
        .replace(/dados pessoais/gi, '[DADOS REMOVIDOS]')
        .replace(/email:.+/gi, '[EMAIL REMOVIDO]')
        .substring(0, 8000); // Limitar tamanho
}
```

## üìà Escalabilidade

### 9.1 Multiplas Inst√¢ncias

```javascript
// Configura√ß√£o para m√∫ltiplos workers
const ollamaWorkers = [
    'http://localhost:11434',
    'http://localhost:11435',
    'http://localhost:11436'
];

class OllamaLoadBalancer {
    async generateEmbedding(text) {
        const worker = this.getNextWorker();
        return await this.callWorker(worker, 'embeddings', { text });
    }
}
```

### 9.2 Estrat√©gias de Cache

```javascript
// Cache multin√≠vel
const cache = {
    // Mem√≥ria (r√°pido, vol√°til)
    memory: new Map(),

    // Redis (r√°pido, persistente)
    redis: redisClient,

    // Arquivo (lento, permanente)
    file: fileCache
};
```

## üéì Exemplos de Uso Avan√ßado

### 10.1 Sistema de Recomenda√ß√£o

```javascript
// An√°lise de desempenho do usu√°rio
const desempenho = await analisarDesempenhoUsuario(userId);

// Recomendar livros baseados em gaps
const recomendacoes = await gerarRecomendacoes(
    desempenho.materiasBaixoDesempenho,
    desempenho.tempoDisponivel
);
```

### 10.2 Gera√ß√£o de Plano de Estudos

```javascript
// Gerar plano personalizado
const plano = await ollamaService.generateStudyPlan({
    objetivo: "Aprovar no ENEM com 800+ em matem√°tica",
    tempoDisponivel: "2 horas/dia",
    nivelAtual: "intermediario",
    pontosFracos: ["geometria", "estatistica"]
});
```

## üìö Recursos Adicionais

### Documenta√ß√£o Oficial
- [Ollama Docs](https://docs.ollama.ai/)
- [Modelos Dispon√≠veis](https://ollama.ai/library)
- [Exemplos de Uso](https://docs.ollama.ai/examples/)

### Modelos Recomendados
- **nomic-embed-text**: Melhor para embeddings em portugu√™s
- **llama3.1:8b**: Balanceado para chat e an√°lise
- **llama3.1:70b**: Para respostas mais sofisticadas (mais pesado)

### Comunidade
- [Discord Ollama](https://discord.gg/ollama)
- [GitHub Issues](https://github.com/jmorganca/ollama/issues)
- [Reddit r/Ollama](https://reddit.com/r/ollama)

## üîÑ Atualiza√ß√µes e Manuten√ß√£o

### Verificar Novas Vers√µes
```bash
# Atualizar Ollama
ollama update

# Verificar modelos atualizados
ollama pull nomic-embed-text  # Re-puxar modelo
```

### Backup de Configura√ß√µes
```bash
# Exportar modelos
ollama list > modelos-backup.txt

# Reimportar se necess√°rio
cat modelos-backup.txt | xargs -I {} ollama pull {}
```

---

## üéâ Conclus√£o

A integra√ß√£o com Ollama transforma o **Seu Estudo** em uma plataforma verdadeiramente inteligente e privada:

- **üîí Privacidade Total** - Dados processados localmente
- **‚ö° Performance Superior** - Respostas instant√¢neas
- **üß† Intelig√™ncia Avan√ßada** - Compreens√£o contextual de livros
- **üìà Escalabilidade** - Controle total sobre recursos
- **üí∞ Custo Zero** - Sem depend√™ncia de APIs pagas

**üöÄ Seu Estudo com Ollama - Educa√ß√£o inteligente, privada e gratuita!**

*√öltima atualiza√ß√£o: 30 de Setembro de 2025*